#!/bin/bash
#
# Copyright 2016 (c) BlueData Software, Inc.
#
#

set -o pipefail
SELF=$(readlink -nf $0)
export CONFIG_BASE_DIR=$(dirname ${SELF})

source ${CONFIG_BASE_DIR}/logging.sh
source ${CONFIG_BASE_DIR}/utils.sh

if [[ "$1" == "--addnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--delnodes" ]]; then
    ## Nothing to do on the existing nodes when we receive this notification.
    exit 0
elif [[ "$1" == "--configure" ]]; then
    log "Starting configuration ... "

    ## Fall through to start the configuration.
else
    echo "ERROR: Unknown command line option(s): '$@'"
    exit 10
fi

source ${CONFIG_BASE_DIR}/macros.sh

####################### AUTOGENERATED CODE STARTS BELOW #######################
#
[ ! -d '/etc/jupyterhub' ] && mkdir -vp /etc/jupyterhub
cp -rvf ${CONFIG_BASE_DIR}/jupyterhub_config.py /etc/jupyterhub/jupyterhub_config.py

[ ! -d '/etc/sudoers.d' ] && mkdir -vp /etc/sudoers.d
cp -rvf ${CONFIG_BASE_DIR}/jupyter /etc/sudoers.d/

[ ! -d '/usr/local/share/jupyter/kernels/apache_toree_pyspark' ] && mkdir -vp /usr/local/share/jupyter/kernels/apache_toree_pyspark
cp -rvf ${CONFIG_BASE_DIR}/p_kernel.json /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json

[ ! -d '/usr/local/share/jupyter/kernels/apache_toree_sql' ] && mkdir -vp /usr/local/share/jupyter/kernels/apache_toree_sql
cp -rvf ${CONFIG_BASE_DIR}/sq_kernel.json /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
    cp -rvf ${CONFIG_BASE_DIR}/spark/spark-defaults.conf /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
    cp -rvf ${CONFIG_BASE_DIR}/spark/spark-env.sh /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/etc/init.d' ] && mkdir -vp /etc/init.d
    cp -rvf ${CONFIG_BASE_DIR}/spark/spark-master /etc/init.d/
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/etc/init.d' ] && mkdir -vp /etc/init.d
    cp -rvf ${CONFIG_BASE_DIR}/spark/spark-slave /etc/init.d/
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf' ] && mkdir -vp /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf
    cp -rvf ${CONFIG_BASE_DIR}/core-site.xml /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/core-site.xml
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/usr/bin' ] && mkdir -vp /usr/bin
    cp -rvf ${CONFIG_BASE_DIR}/hadoop /usr/bin/hadoop
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    [ ! -d '/opt/bluedata/vagent/guestconfig/appconfig' ] && mkdir -vp /opt/bluedata/vagent/guestconfig/appconfig
    cp -rvf ${CONFIG_BASE_DIR}/appjob /opt/bluedata/vagent/guestconfig/appconfig/appjob
fi

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json GET_SERVICE_URL spark_master controller

REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json GET_SERVICE_URL spark_master controller

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf GET_SERVICE_URL spark_master controller
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_MAX_CORES@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-defaults.conf GET_TOTAL_VCORES
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@MASTER_HOST@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh GET_FQDN_LIST controller
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@MEMORY@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh echo $(GET_TOTAL_VMEMORY_MB)m
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@CORES@@@@ /usr/lib/spark/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh GET_TOTAL_VCORES
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-slave GET_NODE_FQDN
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-slave echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_MASTER@@@@ /etc/init.d/spark-slave GET_SERVICE_URL spark_master controller
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@FQDN@@@@ /etc/init.d/spark-master GET_FQDN_LIST controller
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-master echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    REPLACE_PATTERN @@@@SPARK_HOME@@@@ /etc/init.d/spark-master echo /usr/lib/spark/spark-2.2.1-bin-hadoop2.7
fi

if  [[ "${ROLE}" == 'controller' ]] ||\
  [[ "${ROLE}" == 'worker' ]]; then
    bash ${CONFIG_BASE_DIR}/total_vcores.sh || exit 2
fi


REGISTER_START_SERVICE_SYSCTL spark_master spark-master
REGISTER_START_SERVICE_SYSCTL spark_worker spark-slave
REGISTER_START_SERVICE_SYSCTL jupyterhub jupyterhub
