#!/bin/bash
#
# Copyright (c) 2016, BlueData Software, Inc.
#

SELF=$(readlink -nf $0)
export CONFIG_BASE_DIR=$(dirname ${SELF})

JOBID=''
JOBTYPE=''
JOBCMDLINE=''

JOBSTART='false'

parse_args() {
    while getopts ":si:t:c:" opt; do
        case ${opt} in
            s)
                JOBSTART='true'
                ;;
            i)
                JOBID=${OPTARG}
                ;;
            t)
                JOBTYPE=${OPTARG}
                ;;
            c)
                shift $((OPTIND - 2))
                JOBCMDLINE=$@
                ;;
        esac
    done

    if [[ -z ${JOBID} || -z ${JOBTYPE} || -z ${JOBCMDLINE} ]]; then
        echo "ERROR: -i, -t and -c command line options are mandatory."
        exit 1
    fi
}
parse_args $@

APPJOB_LOG_DIR=/var/log/bluedata/
[[ ! -e ${APPJOB_LOG_DIR} ]] && mkdir -p ${APPJOB_LOG_DIR}

export LOG_FILE_PATH="${APPJOB_LOG_DIR}/${JOBID}.out"
source ${CONFIG_BASE_DIR}/logging.sh
source ${CONFIG_BASE_DIR}/utils.sh

NODEGROUP="$(invoke_bdvcli --get node.nodegroup_id)"

################################################################################
#           Add application specific job invocation code below.                #
################################################################################


spark_submit() {
    SPARK_SUBMIT='sudo -u spark JAVA_HOME=/usr/java/jdk1.7.0_67-cloudera /opt/cloudera/parcels/CDH/lib/spark/bin/spark-submit'
    SPARK_MASTER_URI="$(invoke_bdvcli --get services.spark_master.${NODEGROUP}.controller.endpoints)"
    SPARK_MASTER_URI=${SPARK_MASTER_URI%/}
    log_exec ${SPARK_SUBMIT} --master yarn-cluster ${JOBCMDLINE}
}

# Master switch for various jobs types this distro can handle.
if [[ "${JOBTYPE}" == 'hadoopcustom' ]]; then
    log_exec sudo -u hdfs -E hadoop jar ${JOBCMDLINE}
elif [[ "${JOBTYPE}" == 'hadoopstreaming' ]]; then
    STREAMJAR="$(invoke_bdvcli --get cluster.config_metadata.${NODEGROUP}.streaming_jar)"
    log_exec sudo -u hdfs -E hadoop jar ${STREAMJAR} ${JOBCMDLINE}
elif [[ "${JOBTYPE}" == 'pig' ]]; then
    log_exec "sudo -u pig -E pig -f ${JOBCMDLINE}"
elif [[ "${JOBTYPE}" == 'hive' ]]; then
    log_exec "sudo -u hive -E hive -f ${JOBCMDLINE}"
elif [[ "${JOBTYPE}" == 'hbase' ]]; then
    log_exec sudo -u hbase -E hbase shell --noninteractive < ${JOBCMDLINE}
elif [[ "${JOBTYPE}" == 'impala' ]]; then
    log_exec sudo -u impala -E impala-shell -f ${JOBCMDLINE}
elif [[ "${JOBTYPE}" == 'sparkscala' || "${JOBTYPE}" == 'sparkjava' ||\
        "${JOBTYPE}" == 'sparkpython' ]]; then
    spark_submit
else
    log_error "Unknow Job type: ${JOBTYPE}"
    exit 3
fi

exit 0;
